# -*- coding: utf-8 -*-
"""P3-ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4RMBhgU4ytPcVtdTLArJe6UVTdjRaUO

## Carga y Exploración de Datos
"""

!pip install umap-learn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
import umap
import plotly.express as px

# Configuraciones para visualización
sns.set(style="whitegrid")

# Cargar el dataset
url = "https://raw.githubusercontent.com/aljozu/p3-ML/refs/heads/main/heart.csv"
df = pd.read_csv(url, header=0)

# Convertir a numérico, valores inválidos se transforman en NaN
for col in df.columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Mostrar si hay valores faltantes
print(df.isnull().sum())

# Eliminar filas con valores nulos (Paso opcional)
df = df.dropna()

# Estadísticas descriptivas
print(df.describe())

# Visualizar la distribución de las variables
df.hist(bins=15, figsize=(15, 10))
plt.suptitle("Distribuciones de las variables")
plt.show()

# Correlación entre variables
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Matriz de correlación entre variables")
plt.show()

"""## Preprocesamiento"""

# Escalar los datos (normalización)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.drop(columns=['target']))  # Excluir 'target' ya que es la variable de salida

# Aplicar T-SNE
tsne = TSNE(n_components=2, random_state=42)
tsne_result = tsne.fit_transform(df_scaled)

# Visualizar el resultado de T-SNE
plt.figure(figsize=(8, 6))
sns.scatterplot(x=tsne_result[:, 0], y=tsne_result[:, 1], hue=df['target'], palette='coolwarm', alpha=0.7)
plt.title("Visualización de T-SNE")
plt.xlabel('Componente  1')  # Etiqueta del eje X
plt.ylabel('Componente  2')  # Etiqueta del eje Y
plt.show()

# Aplicar UMAP
umap_model = umap.UMAP(n_components=2, random_state=42)
umap_result = umap_model.fit_transform(df_scaled)

# Visualizar el resultado de UMAP
plt.figure(figsize=(8, 6))
sns.scatterplot(x=umap_result[:, 0], y=umap_result[:, 1], hue=df['target'], palette='coolwarm', alpha=0.7)
plt.title("Visualización de UMAP")
plt.xlabel('Componente 1')  # Etiqueta del eje X
plt.ylabel('Componente  2')  # Etiqueta del eje Y
plt.show()

from sklearn.decomposition import PCA

# Aplicar PCA para reducir a 2 componentes
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df.drop(columns=['target']))

# Visualizar la varianza explicada por cada componente
print(f"Varianza explicada por cada componente: {pca.explained_variance_ratio_}")

# Visualizar los datos reducidos
scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=df['target'], cmap='coolwarm')
plt.title("PCA - Reducción de dimensionalidad")
plt.xlabel('Componente principal 1')  # Etiqueta del eje X
plt.ylabel('Componente principal 2')  # Etiqueta del eje Y
plt.colorbar(scatter, label='Target (0 = Ausente, 1 = Presente)')  # Barra de color para la leyenda
plt.show()

"""## Algoritmos

### K-means
"""

import numpy as np
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score

# K-means
class KMeansClustering:
    def __init__(self, X, num_clusters, max_iterations=100, tol=1e-4, random_state=42):
        self.K = num_clusters  # Número de clústeres
        self.max_iterations = max_iterations  # Número máximo de iteraciones
        self.tol = tol  # Tolerancia para la convergencia
        self.num_examples, self.num_features = X.shape  # Número de ejemplos y características
        self.random_state = random_state  # Semilla para reproducibilidad
        np.random.seed(self.random_state)

    # Inicializar centroides aleatoriamente
    def initialize_random_centroids(self, X):
        random_indices = np.random.choice(self.num_examples, self.K, replace=False)
        centroids = X[random_indices]
        return centroids

    # Asignar puntos al clúster más cercano
    def create_cluster(self, X, centroids):
        distances = cdist(X, centroids, 'euclidean')  # Cálculo eficiente de distancias
        closest_centroids = np.argmin(distances, axis=1)
        clusters = [[] for _ in range(self.K)]
        for idx, centroid_idx in enumerate(closest_centroids):
            clusters[centroid_idx].append(idx)
        return clusters

    # Recalcular centroides
    def calculate_new_centroids(self, clusters, X):
        centroids = np.zeros((self.K, self.num_features))
        for cluster_idx, cluster in enumerate(clusters):
            if cluster:  # Si el clúster no está vacío
                centroids[cluster_idx] = np.mean(X[cluster], axis=0)
            else:
                # Manejar clústeres vacíos re-inicializando el centroide
                centroids[cluster_idx] = X[np.random.choice(self.num_examples)]
        return centroids

    # Predecir clústeres
    def predict_cluster(self, clusters, X):
        y_pred = np.zeros(self.num_examples)
        for cluster_idx, cluster in enumerate(clusters):
            for sample_idx in cluster:
                y_pred[sample_idx] = cluster_idx
        return y_pred

    # Ajustar el algoritmo KMeans
    def fit(self, X):
        centroids = self.initialize_random_centroids(X)
        for iteration in range(self.max_iterations):
            clusters = self.create_cluster(X, centroids)
            previous_centroids = centroids
            centroids = self.calculate_new_centroids(clusters, X)
            # Verificar convergencia
            if np.all(np.abs(centroids - previous_centroids) < self.tol):
                break
        y_pred = self.predict_cluster(clusters, X)
        return y_pred, centroids

# Método del codo para encontrar el número óptimo de clústeres
silhouette_scores = []
k_range = range(2, 10)
for k in k_range:
    kmeans = KMeansClustering(df_pca, num_clusters=k)
    y_pred, _ = kmeans.fit(df_pca)
    score = silhouette_score(df_pca, y_pred)
    silhouette_scores.append((k, score))

# Gráfico de los puntajes de silueta
ks, scores = zip(*silhouette_scores)
plt.plot(ks, scores, marker='o')
plt.title("Silhouette Score vs Number of Clusters")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.grid()
plt.show()

# Ajustar K-means con el número óptimo de clústeres
optimal_k = ks[np.argmax(scores)]
kmeans = KMeansClustering(df_pca, num_clusters=optimal_k)
y_pred, centroids = kmeans.fit(df_pca)

# Visualizar los clústeres obtenidos
plt.figure(figsize=(8, 6))
scatter = plt.scatter(df_pca[:, 0], df_pca[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.7)
plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='Centroides')  # Marcar los centros de los clústeres
plt.title("Clústeres obtenidos con K-means después de PCA")
plt.xlabel('Componente principal 1')
plt.ylabel('Componente principal 2')

# Agregar leyenda
handles, labels = scatter.legend_elements()
plt.legend(handles, labels, title="Clústeres")

plt.show()

"""### DBScan"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score, confusion_matrix, ConfusionMatrixDisplay

# Clase CDBSCAN para clustering basado en densidad
class CDBSCAN:
    def __init__(self, eps: float, min_samples: int):
        self.eps = eps
        self.min_samples = min_samples
        self.labels = None
        self.cluster_id = 0

    def fit(self, data: np.ndarray):
        self.labels = np.full(data.shape[0], -1)
        for point_index in range(data.shape[0]):
            if self.labels[point_index] != -1:
                continue
            neighbors = self.region_query(data, point_index)
            if len(neighbors) < self.min_samples:
                self.labels[point_index] = -1
            else:
                self.cluster_id += 1
                self.expand_cluster(data, point_index, neighbors)
        return self.labels

    def expand_cluster(self, data: np.ndarray, point_index: int, neighbors: list):
        self.labels[point_index] = self.cluster_id
        i = 0
        while i < len(neighbors):
            neighbor_index = neighbors[i]
            if self.labels[neighbor_index] == -1:
                self.labels[neighbor_index] = self.cluster_id
            if self.labels[neighbor_index] == -1 or self.labels[neighbor_index] == 0:
                self.labels[neighbor_index] = self.cluster_id
                new_neighbors = self.region_query(data, neighbor_index)
                if len(new_neighbors) >= self.min_samples:
                    neighbors.extend(new_neighbors)
            i += 1

    def region_query(self, data: np.ndarray, point_index: int) -> list:
        distances = np.linalg.norm(data - data[point_index], axis=1)
        return np.where(distances < self.eps)[0].tolist()

# Evaluación del coeficiente de silueta para diferentes valores de eps
eps_values = np.arange(0.1, 200, 10)
silhouette_scores = []

for eps in eps_values:
    dbscan = CDBSCAN(eps=eps, min_samples=5)
    labels = dbscan.fit(df_pca)
    if len(set(labels)) > 1:
        score = silhouette_score(df_pca, labels)
        silhouette_scores.append(score)
    else:
        silhouette_scores.append(-1)

# Gráfico del Coeficiente de Silueta vs. eps
plt.figure(figsize=(10, 6))
plt.plot(eps_values, silhouette_scores, marker='o', linestyle='-', color='b')
plt.title('Coeficiente de Silueta vs. eps en DBSCAN')
plt.xlabel('Valor de eps')
plt.ylabel('Coeficiente de Silueta')
plt.xticks(np.arange(0, 200, 20))
plt.grid()
plt.show()

# Determinar el valor óptimo de eps
optimal_eps = eps_values[np.argmax(silhouette_scores)]


# Ajuste final de DBSCAN con el eps óptimo
dbscan = CDBSCAN(eps=optimal_eps, min_samples=5)
y_pred = dbscan.fit(df_pca)

# Calcular los centroides de los clústeres
centroides = []
for cluster_id in np.unique(y_pred):
    if cluster_id != -1:
        cluster_points = df_pca[y_pred == cluster_id]
        centroid = cluster_points.mean(axis=0)
        centroides.append(centroid)

centroides = np.array(centroides)

# Visualización de clústeres y centroides
plt.figure(figsize=(8, 6))
for cluster_id in np.unique(y_pred):
    if cluster_id == -1:
        plt.scatter(df_pca[y_pred == cluster_id, 0], df_pca[y_pred == cluster_id, 1],
                    c='red', s=50, marker='o', label='Ruido')
    else:
        plt.scatter(df_pca[y_pred == cluster_id, 0], df_pca[y_pred == cluster_id, 1],
                    s=50, alpha=0.7, label=f'Clúster {cluster_id}')

plt.scatter(centroides[:, 0], centroides[:, 1], s=300, c='black', marker='X', label='Centroides')
plt.title(f"Clústeres obtenidos con DBSCAN")
plt.xlabel('Componente principal 1')
plt.ylabel('Componente principal 2')
plt.legend(title='Grupos identificados')
plt.show()

"""### Mean Shift"""

from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.metrics import silhouette_score
import numpy as np
import matplotlib.pyplot as plt

# Inicializar variables
best_bandwidth = None
best_silhouette_score = -1  # Comenzar con la puntuación más baja posible
best_labels = None
quantiles = [0.1, 0.2, 0.3, 0.4, 0.5]
silhouette_scores = []

# Iterar sobre diferentes cuantiles para encontrar el mejor ancho de banda
for quantile in quantiles:
    bandwidth = estimate_bandwidth(df_pca, quantile=quantile)
    mean_shift = MeanShift(bandwidth=bandwidth)
    mean_shift.fit(df_pca)

    # Calcular etiquetas y puntuación de silueta
    labels = mean_shift.labels_
    if len(set(labels)) > 1:  # Asegurar que hay más de un clúster
        silhouette_avg = silhouette_score(df_pca, labels)
        print(f"Quantile: {quantile}, Bandwidth: {bandwidth:.2f}, Silhouette Score: {silhouette_avg:.2f}")

        # Rastrear el mejor ancho de banda basado en la puntuación de silueta
        if silhouette_avg > best_silhouette_score:
            best_silhouette_score = silhouette_avg
            best_bandwidth = bandwidth
            best_labels = labels
        silhouette_scores.append(silhouette_avg)

# Gráfico del coeficiente de silueta
plt.figure(figsize=(10, 6))
plt.plot(quantiles, silhouette_scores, marker='o', linestyle='-', color='b')
plt.title('Coeficiente de Silueta vs. Quantiles')
plt.xlabel('Valor de Cuantil')
plt.ylabel('Coeficiente de Silueta')
plt.xticks(quantiles)
plt.grid()
plt.show()

# Resultado final
print(f"\nBest Bandwidth: {best_bandwidth:.2f}, Best Silhouette Score: {best_silhouette_score:.2f}")

# Estimar el ancho de banda
mean_shift = MeanShift(bandwidth=best_bandwidth)
mean_shift.fit(df_pca)

# Centros de clúster
centers = mean_shift.cluster_centers_
print("Cluster Centers:\n", centers)

# Etiquetas de cada punto
labels = mean_shift.labels_
print("Labels:", labels)

# Graficar puntos de datos con etiquetas de clúster
plt.figure(figsize=(10, 6))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=labels, cmap='viridis', marker='o', label="Puntos de Datos")
plt.scatter(centers[:, 0], centers[:, 1], s=300, c='red', marker='X', label="Centros de Clúster")
plt.title("Clustering con Mean Shift")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.legend()
plt.show()

"""### Gaussian Mixture Model"""

from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
import numpy as np
import matplotlib.pyplot as plt

# Inicializar variables
best_n_components = None
best_silhouette_score = -1
best_labels = None
n_components_range = range(2, 10)  # Ajustar el rango según sea necesario
silhouette_scores = []  # Lista para almacenar los puntajes de silueta

# Iterar sobre diferentes números de componentes para encontrar el mejor
for n_components in n_components_range:
    gmm = GaussianMixture(n_components=n_components, random_state=42)
    gmm.fit(df_pca)

    # Obtener las etiquetas del modelo GMM
    labels = gmm.predict(df_pca)

    # Calcular el puntaje de silueta (solo si hay más de un clúster)
    if len(set(labels)) > 1:
        silhouette_avg = silhouette_score(df_pca, labels)
        silhouette_scores.append(silhouette_avg)  # Agregar puntaje a la lista
        print(f"Número de Componentes: {n_components}, Silhouette Score: {silhouette_avg:.2f}")

        # Rastrear el mejor número de componentes
        if silhouette_avg > best_silhouette_score:
            best_silhouette_score = silhouette_avg
            best_n_components = n_components
            best_labels = labels

# Resultados finales
print(f"\nMejor Número de Componentes: {best_n_components}, Mejor Silhouette Score: {best_silhouette_score:.2f}")

# Gráfico de los puntajes de silueta
plt.figure(figsize=(10, 6))
plt.plot(n_components_range, silhouette_scores, marker='o', linestyle='-', color='b')
plt.title('Coeficiente de Silueta vs. Número de Componentes')
plt.xlabel('Número de Componentes')
plt.ylabel('Coeficiente de Silueta')
plt.xticks(n_components_range)
plt.grid()
plt.show()

# Ajustar GMM con el número óptimo de componentes
gmm = GaussianMixture(n_components=best_n_components, random_state=42)
gmm.fit(df_pca)

# Centros de clústeres
centers = gmm.means_
print("Cluster Centers:\n", centers)

# Etiquetas de cada punto
labels = gmm.predict(df_pca)
print("Labels:", labels)

# Graficar puntos de datos con etiquetas de clúster
plt.figure(figsize=(10, 6))

# Usar un bucle para graficar cada clúster con su propio color
unique_labels = np.unique(labels)
for label in unique_labels:
    plt.scatter(df_pca[labels == label, 0], df_pca[labels == label, 1],
                label=f"Cluster {label}", marker='o', alpha=0.5)  # Cambia el marcador si es necesario

# Graficar los centroides sin confusión
plt.scatter(centers[:, 0], centers[:, 1], s=300, c='red', marker='X', label="Cluster Centers", edgecolor='k')

plt.title("Clustering GMM")
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.legend()
plt.show()